# -*- coding: utf-8 -*-
"""Moving Data Processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LttfL6PKbT1DdG9dye8AyFT0yMes9X4X
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import seaborn as sns
import numpy as np

data = pd.read_csv("/content/drive/My Drive/covid project/Data/13-Dec.csv",index_col=0)
data.iloc[-1:, -4:] = data.iloc[1:-1,-4:].astype(int).sum().to_list()

total_cases = pd.DataFrame()
total_cured = pd.DataFrame()
total_deaths = pd.DataFrame()

i=1
for col in data.columns:
    if i%4==1:
        total_cases[col]=data[col]
    if i%4==3:total_cured[col]=data[col]
    if i%4==0:total_deaths[col]=data[col]
    i+=1

total_cases.drop(index='NUMBERS', axis=0, inplace=True)
total_cured.drop(index='NUMBERS', axis=0, inplace=True)
total_deaths.drop(index='NUMBERS', axis=0, inplace=True)
total_cases = total_cases.astype(int)
total_cured = total_cured.astype(int)
total_deaths = total_deaths.astype(int)

from datetime import timedelta, date

def daterange(date1, date2):
    for n in range(int ((date2 - date1).days)+1):
        yield date1 + timedelta(n)

start_dt = date(2020, 4, 17)
end_dt = date(2020, 12, 1)
time_axis = []
for dt in daterange(start_dt, end_dt):
    time_axis.append(dt)
date= pd.to_datetime(time_axis)
date.shape

tot = total_cases.copy().transpose()
# tot.loc["26-Sep",["Andhra Pradesh", "Andaman and Nicobar Islands"]] = tot.loc["26-Sep",["Andaman and Nicobar Islands", "Andhra Pradesh"]].values
tot.loc["26-Sep"]

new_cases = tot.copy()
for row in range(1,len(tot.index)):
    new_cases.iloc[row,0:tot.shape[1]] = (tot.iloc[row,0:tot.shape[1]]-tot.iloc[row-1,0:tot.shape[1]]).astype(int)
new_cases

# other_states = [col for col in cases.columns if col not in ["Delhi", "Maharashtra", "Tamil Nadu","Andhra Pradesh", "Totals"]]
# cases["Rest of India"] = cases[other_states].sum(axis=1)
# cases
# curr_pars = new_cases[["Maharashtra", "Delhi", "Tamil Nadu","Gujarat", "Uttar Pradesh", "Rajasthan", "West Bengal", "Madhya Pradesh", "Haryana", "Karnataka","Totals"]]
other_states = [col for col in new_cases.columns if col not in ["Andhra Pradesh", "Maharashtra", "Tamil Nadu", "Karnataka", "Totals"]]
new_cases["Rest of India"] = new_cases[other_states].sum(axis=1)
curr_pars = new_cases.copy()
# curr_pars
curr_pars = curr_pars.iloc[36:-11]
curr_pars

curr_pars.loc["26-Sep"]

mov_avg = curr_pars.rolling(3).mean()
mov_avg = mov_avg.iloc[2:]
mov_avg

from pandas import Series
from sklearn.preprocessing import MinMaxScaler
from sklearn.externals import joblib
def new_cases_df(name,values):
  # define contrived series
  # series = Series(state_df)
  # print(series)
  # prepare data for normalization
  # values = series.values
  values = values.reshape((len(values), 1))
  # train the normalization
  scaler = MinMaxScaler(feature_range=(0, 1))
  scaler = scaler.fit(values)
  scaler_filename = "/content/drive/My Drive/covid project/Data/scalars/" + name + "_moving"
  joblib.dump(scaler, scaler_filename) 
  print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))
  # normalize the dataset and print
  normalized = scaler.transform(values)
  # print(normalized)
  # inverse transform and print
  inversed = scaler.inverse_transform(normalized)
  # print(inversed)
  return normalized

def per_cases(values):
  # define contrived series
  # series = Series(state_df)
  # print(series)
  # prepare data for normalization
  # values = series.values
  values = values.reshape((len(values), 1))
  # print('Min: %f, Max: %f' % (min(values), max(values)))
  # train the normalization
  scaler = MinMaxScaler(feature_range=(0, 1))
  scaler = scaler.fit(values)
  print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))
  # normalize the dataset and print
  normalized = scaler.transform(values)
  # print(normalized)
  # inverse transform and print
  inversed = scaler.inverse_transform(normalized)
  # # print(inversed)
  return  normalized,inversed

def split_wall(df_frame):
  count = 1
  for date in df_frame['date']:
      if (str(date) == "2020-09-30 00:00:00"):
          break
      count += 1
  return count

def split_sequence(sequence, n_steps_in, n_steps_out):
    X, y = list(), list()
    for i in range(len(sequence)):
        # find the end of this pattern
        end_ix = i + n_steps_in
        out_end_ix = end_ix + n_steps_out
        # check if we are beyond the sequence
        if out_end_ix > len(sequence):
            break
        # gather input and output parts of the pattern
        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X),np.array(y)

def train_and_test(count, normalized):
  data = np.array(normalized)
  data = data.reshape((data.shape[0]))
  train_data = data[:count]
  # Remaining is test data
  test_data  = data[count:]
  # print(test_data)
  # choose a number of time steps
  n_steps_in, n_steps_out = 6,4

  # split into samples
  train_X, train_Y = split_sequence(train_data, n_steps_in, n_steps_out)
  train = np.concatenate((train_X, train_Y), axis=1)
  clm_names = []
  for i in range(1,n_steps_in+1):
    clm_names.append('Input' + str(i))
  for i in range(1,n_steps_out+1):
    clm_names.append('Output' + str(i))
  train = pd.DataFrame(train, columns = clm_names)
  # print(train.head())

  #summarize the data
  # for i in range(len(train_X)):
  #   print(train_X[i], train_Y[i])

  # split into samples
  test_X, test_Y= split_sequence(test_data, n_steps_in, n_steps_out)

  test = np.concatenate((test_X, test_Y), axis=1)
  test = pd.DataFrame(test, columns = clm_names)
  # print(test.head())

  # summarize the data
  # for i in range(len(test_X)):
  #     print(test_X[i], test_Y[i])
    
  return train,test

# GENERATING DATA

for state_name in list(mov_avg):
  print(state_name)
  normalized = new_cases_df(state_name,mov_avg[state_name].values)
  mov_avg['date'] = date
  count = split_wall(mov_avg)
  print (count)
  train,test = train_and_test(count,normalized)
  train_str_name = "train_" + state_name
  test_str_name = "test_" + state_name
  train.to_csv('/content/drive/My Drive/covid project/Data/Processed_Data/moving_avg/' + train_str_name +'.csv')
  test.to_csv('/content/drive/My Drive/covid project/Data/Processed_Data/moving_avg/' + test_str_name +'.csv')



# shuffled_moving_avg

from sklearn.model_selection import ShuffleSplit
rs = ShuffleSplit(n_splits=1, test_size=.25, random_state=0)

def shuffled_test_train(n_inp, n_out, sequence):
  batches = []
  for i in range(len(sequence)):
    if (i + n_inp + n_out > len(sequence)):
      break
    batches.append(sequence[i:i + n_inp + n_out])
  
  train = []
  test = []
  for train_index, test_index in rs.split(batches):
    print("TRAIN:", train_index, "TEST:", test_index)
    for i in train_index:
      train.append(batches[i])
    for i in test_index:
      test.append(batches[i])

  return train,test
  
for state_name in list(mov_avg):
  print(state_name)
  normalized = new_cases_df(state_name,mov_avg[state_name].values)
  normalized = normalized.reshape((normalized.shape[0]))
  # print(normalized)
  n_inp = 6
  n_out = 4
  train,test = shuffled_test_train(n_inp, n_out, normalized)
  clm_names = []
  for j in range(1,n_inp+1):
    clm_names.append('Input' + str(j))
  for j in range(1,n_out+1):
    clm_names.append('Output' + str(j))
  train = pd.DataFrame(train, columns = clm_names) 
  test = pd.DataFrame(test, columns = clm_names)
  print(train)
  print(test)
  train_str_name = "train_" + state_name
  test_str_name = "test_" + state_name
  train.to_csv('/content/drive/My Drive/covid project/Data/Processed_Data/random_shuffle/' + train_str_name +'.csv')
  test.to_csv('/content/drive/My Drive/covid project/Data/Processed_Data/random_shuffle/' + test_str_name +'.csv')

